{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "First of all, make sure your environment has installed the latest version of [ðŸ¤— Optimum Graphcore](https://github.com/huggingface/optimum-graphcore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.python.org/simple/\n",
      "Collecting git+https://github.com/huggingface/optimum-graphcore.git\n",
      "  Cloning https://github.com/huggingface/optimum-graphcore.git to /tmp/pip-req-build-vdejgdgn\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/optimum-graphcore.git /tmp/pip-req-build-vdejgdgn\n",
      "  Resolved https://github.com/huggingface/optimum-graphcore.git to commit 9ecdc44a1342acc75ac368044609556df22e5b0e\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tokenizers in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.4.4.dev0) (0.11.1)\n",
      "Requirement already satisfied: torch in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.4.4.dev0) (1.10.0)\n",
      "Requirement already satisfied: scipy in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.4.4.dev0) (1.9.3)\n",
      "Requirement already satisfied: transformers==4.20.1 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.4.4.dev0) (4.20.1)\n",
      "Requirement already satisfied: sentencepiece in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.4.4.dev0) (0.1.97)\n",
      "Requirement already satisfied: pillow in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.4.4.dev0) (9.3.0)\n",
      "Requirement already satisfied: datasets in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.4.4.dev0) (2.7.1)\n",
      "Requirement already satisfied: optimum in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.4.4.dev0) (1.5.1)\n",
      "Requirement already satisfied: filelock in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from transformers==4.20.1->optimum-graphcore==0.4.4.dev0) (3.8.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from transformers==4.20.1->optimum-graphcore==0.4.4.dev0) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from transformers==4.20.1->optimum-graphcore==0.4.4.dev0) (22.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from transformers==4.20.1->optimum-graphcore==0.4.4.dev0) (0.11.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from transformers==4.20.1->optimum-graphcore==0.4.4.dev0) (2022.10.31)\n",
      "Requirement already satisfied: numpy>=1.17 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from transformers==4.20.1->optimum-graphcore==0.4.4.dev0) (1.23.5)\n",
      "Requirement already satisfied: requests in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from transformers==4.20.1->optimum-graphcore==0.4.4.dev0) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from transformers==4.20.1->optimum-graphcore==0.4.4.dev0) (6.0)\n",
      "Requirement already satisfied: pandas in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore==0.4.4.dev0) (1.5.2)\n",
      "Requirement already satisfied: dill<0.3.7 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore==0.4.4.dev0) (0.3.6)\n",
      "Requirement already satisfied: multiprocess in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore==0.4.4.dev0) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore==0.4.4.dev0) (2022.11.0)\n",
      "Requirement already satisfied: aiohttp in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore==0.4.4.dev0) (3.8.3)\n",
      "Requirement already satisfied: responses<0.19 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore==0.4.4.dev0) (0.18.0)\n",
      "Requirement already satisfied: xxhash in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore==0.4.4.dev0) (3.1.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore==0.4.4.dev0) (10.0.1)\n",
      "Requirement already satisfied: coloredlogs in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from optimum->optimum-graphcore==0.4.4.dev0) (15.0.1)\n",
      "Requirement already satisfied: sympy in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from optimum->optimum-graphcore==0.4.4.dev0) (1.11.1)\n",
      "Requirement already satisfied: typing-extensions in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from torch->optimum-graphcore==0.4.4.dev0) (4.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore==0.4.4.dev0) (6.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore==0.4.4.dev0) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore==0.4.4.dev0) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore==0.4.4.dev0) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore==0.4.4.dev0) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore==0.4.4.dev0) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore==0.4.4.dev0) (22.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: certifi>=2017.4.17 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from requests->transformers==4.20.1->optimum-graphcore==0.4.4.dev0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from requests->transformers==4.20.1->optimum-graphcore==0.4.4.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from requests->transformers==4.20.1->optimum-graphcore==0.4.4.dev0) (1.26.13)\n",
      "Requirement already satisfied: protobuf<=3.20.1 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from transformers==4.20.1->optimum-graphcore==0.4.4.dev0) (3.20.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from coloredlogs->optimum->optimum-graphcore==0.4.4.dev0) (10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from pandas->datasets->optimum-graphcore==0.4.4.dev0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from pandas->datasets->optimum-graphcore==0.4.4.dev0) (2022.6)\n",
      "Requirement already satisfied: mpmath>=0.19 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from sympy->optimum->optimum-graphcore==0.4.4.dev0) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum-graphcore==0.4.4.dev0) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/huggingface/optimum-graphcore.git;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also make sure all the packages required for text classification are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.python.org/simple/\n",
      "Requirement already satisfied: scikit-learn in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from scikit-learn) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.python.org/simple/\n",
      "Requirement already satisfied: matplotlib in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (3.6.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from matplotlib) (22.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.19 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: six>=1.5 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.python.org/simple/\n",
      "Requirement already satisfied: tokenizers==0.11.1 in /localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (0.11.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install scikit-learn;\n",
    "! pip install matplotlib;\n",
    "! pip install tokenizers==0.11.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the versions of Transformers and Optimum Graphcore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.20.1\n",
      "0.4.4.dev0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import optimum.graphcore\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(optimum.graphcore.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Fine-tuning BERT on a text classification task using packing\n",
    "\n",
    "This notebook is an alternative for [Fine-tuning BERT on a text classification task](text_classification.ipynb) showing how to implement packing for BERT step by step and use if for fine-tuning on `GLUE/sst2` text classification. This includes packing the dataset and adapting an existing BERT model. Packing consists in concatenating several input sequences into one to increase the computational efficiency. More details about packing can be found in the [blog post](https://www.graphcore.ai/posts/introducing-packed-bert-for-2x-faster-training-in-natural-language-processing) and the original [paper](https://arxiv.org/abs/2107.02027)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTCFado4IrIc"
   },
   "source": [
    "In this notebook, we will see how to fine-tune one of the [ðŸ¤— Transformers](https://github.com/huggingface/transformers) model to a text classification task of the [GLUE Benchmark](https://gluebenchmark.com/).\n",
    "\n",
    "![Widget inference on a text classification task](images/text_classification.png)\n",
    "\n",
    "The GLUE Benchmark is a group of nine classification tasks on sentences or pairs of sentences which are:\n",
    "\n",
    "- [CoLA](https://nyu-mll.github.io/CoLA/) (Corpus of Linguistic Acceptability) Determine if a sentence is grammatically correct or not.is a  dataset containing sentences labeled grammatically correct or not.\n",
    "- [MNLI](https://arxiv.org/abs/1704.05426) (Multi-Genre Natural Language Inference) Determine if a sentence entails, contradicts or is unrelated to a given hypothesis. (This dataset has two versions, one with the validation and test set coming from the same distribution, another called mismatched where the validation and test use out-of-domain data.)\n",
    "- [MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398) (Microsoft Research Paraphrase Corpus) Determine if two sentences are paraphrases from one another or not.\n",
    "- [QNLI](https://rajpurkar.github.io/SQuAD-explorer/) (Question-answering Natural Language Inference) Determine if the answer to a question is in the second sentence or not. (This dataset is built from the SQuAD dataset.)\n",
    "- [QQP](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs) (Quora Question Pairs2) Determine if two questions are semantically equivalent or not.\n",
    "- [RTE](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment) (Recognizing Textual Entailment) Determine if a sentence entails a given hypothesis or not.\n",
    "- [SST-2](https://nlp.stanford.edu/sentiment/index.html) (Stanford Sentiment Treebank) Determine if the sentence has a positive or negative sentiment.\n",
    "- [STS-B](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) (Semantic Textual Similarity Benchmark) Determine the similarity of two sentences with a score from 1 to 5.\n",
    "- [WNLI](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html) (Winograd Natural Language Inference) Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not. (This dataset is built from the Winograd Schema Challenge dataset.)\n",
    "\n",
    "We will see how to easily load the dataset for each one of those tasks and use packed BERT to fine-tune a model on it. Each task is named by its acronym, with `mnli-mm` standing for the mismatched version of MNLI (so same training set as `mnli` but different validation and test sets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YZbiBDuGIrId"
   },
   "outputs": [],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For this Packed bert demo, we will cover (single-label) sequence classification on `sst2` dataset. But `task` can be changed to run the other `GLUE` tasks . However, training hyperparameters may need some tuning for these other tasks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RRkXuteIrIh"
   },
   "source": [
    "In this notebook, we are using both data parallelism and pipeline parallelism (see this [tutorial](https://github.com/graphcore/tutorials/tree/master/tutorials/pytorch/tut2_efficient_data_loading) for more). Therefore the global batch size, which is the actual number of samples used for the weight update, is determined with three factors:\n",
    "- global batch size = micro_batch_size * gradient accumulation steps * replication factor\n",
    "\n",
    "and replication factor is determined by `pod_type`, which will be used as a key to select the replication factor from a dictionary defined in the IPU config file. For example, the dictionary in the IPU config file [Graphcore/roberta-base-ipu](https://huggingface.co/Graphcore/roberta-base-ipu/blob/main/ipu_config.json) looks like this:\n",
    "- \"replication_factor\": {\"pod4\": 1, \"pod8\": 2, \"pod16\": 4, \"pod32\": 8, \"pod64\": 16, \"default\": 1}\n",
    "\n",
    "Depending on you model and the pod machine you are using, you might need to adjust these three batch-size-related arguments.\n",
    "\n",
    "By default this notebook is configured to run on 4 IPUs.\n",
    "\n",
    "Finally, `max_seq_length` is the length we are going to pad the sentences to, so it should not be larger than the maximum length of the model. Set those seven parameters, then the rest of the notebook should run smoothly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the small size of the sequences in `sst2`, we can reduce the model input size to `max_seq_length = 256`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zVvslsfMIrIh"
   },
   "outputs": [],
   "source": [
    "task = \"sst2\"\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "ipu_config_name = \"Graphcore/bert-base-uncased\"\n",
    "micro_batch_size = 2\n",
    "gradient_accumulation_steps = 32\n",
    "pod_type = \"pod4\"\n",
    "max_seq_length = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7QYTpxXIrIl"
   },
   "source": [
    "We will use the [ðŸ¤— Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions `load_dataset` and `load_metric`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKx2zKs5IrIq"
   },
   "source": [
    "Apart from `mnli-mm` being a special code, we can directly pass our task name to those functions. `load_dataset` will cache the dataset to avoid downloading it again the next time you run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/alexandrec/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d988d55aa04c818fcbe49ee9e07818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2818717/1389288479.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('glue', actual_task)\n"
     ]
    }
   ],
   "source": [
    "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "dataset = load_dataset(\"glue\", actual_task)\n",
    "metric = load_metric('glue', actual_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzfPtOMoIrIu"
   },
   "source": [
    "The `dataset` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set (with more keys for the mismatched validation and test set in the special case of `mnli`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GWiVUF0jIrIv",
    "outputId": "35e3ea43-f397-4a54-c90c-f2cf8d36873e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3EtYfeHIrIz"
   },
   "source": [
    "To access an actual element, you need to select a split first, then give an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'hide new secretions from the parental units ',\n",
       " 'label': 0,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to suffer '</td>\n",
       "      <td>negative</td>\n",
       "      <td>22714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stylized swedish fillm about a modern city where all the religious and civic virtues that hold society in place are in tatters .</td>\n",
       "      <td>positive</td>\n",
       "      <td>59290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'s still tainted by cliches , painful improbability and murky points</td>\n",
       "      <td>negative</td>\n",
       "      <td>22205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>, sorrow , laugther , and tears</td>\n",
       "      <td>positive</td>\n",
       "      <td>15427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in the way the ivan character accepts the news of his illness so quickly but still finds himself unable to react</td>\n",
       "      <td>negative</td>\n",
       "      <td>26571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>offers big , fat , dumb</td>\n",
       "      <td>negative</td>\n",
       "      <td>39794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>funny in a sick , twisted sort of way .</td>\n",
       "      <td>positive</td>\n",
       "      <td>54178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gorgeous</td>\n",
       "      <td>positive</td>\n",
       "      <td>38780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>that propels her into the upper echelons of the directing world</td>\n",
       "      <td>positive</td>\n",
       "      <td>37590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'s just tediously bad , something to be fully forgotten</td>\n",
       "      <td>negative</td>\n",
       "      <td>46262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnjDIuQ3IrI-"
   },
   "source": [
    "The metric is an instance of [`datasets.Metric`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Metric):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5o4rUteaIrI_",
    "outputId": "18038ef5-554c-45c5-e00a-133b02ec10f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"glue\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
       "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
       "Args:\n",
       "    predictions: list of predictions to score.\n",
       "        Each translation should be tokenized into a list of tokens.\n",
       "    references: list of lists of references for each translation.\n",
       "        Each reference should be tokenized into a list of tokens.\n",
       "Returns: depending on the GLUE subset, one or several of:\n",
       "    \"accuracy\": Accuracy\n",
       "    \"f1\": F1 score\n",
       "    \"pearson\": Pearson Correlation\n",
       "    \"spearmanr\": Spearman Correlation\n",
       "    \"matthews_correlation\": Matthew Correlation\n",
       "Examples:\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0, 'f1': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'stsb')\n",
       "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
       "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'cola')\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'matthews_correlation': 1.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAWdqcUBIrJC"
   },
   "source": [
    "You can call its `compute` method with your predictions and labels directly and it will return a dictionary with the metric(s) value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6XN1Rq0aIrJC",
    "outputId": "a4405435-a8a9-41ff-9f79-a13077b587c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.4375}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "fake_preds = np.random.randint(0, 2, size=(64,))\n",
    "fake_labels = np.random.randint(0, 2, size=(64,))\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOCrQwPoIrJG"
   },
   "source": [
    "Note that `load_metric` has loaded the proper metric associated to your task, which is:\n",
    "\n",
    "- for CoLA: [Matthews Correlation Coefficient](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient)\n",
    "- for MNLI (matched or mismatched): Accuracy\n",
    "- for MRPC: Accuracy and [F1 score](https://en.wikipedia.org/wiki/F1_score)\n",
    "- for QNLI: Accuracy\n",
    "- for QQP: Accuracy and [F1 score](https://en.wikipedia.org/wiki/F1_score)\n",
    "- for RTE: Accuracy\n",
    "- for SST-2: Accuracy\n",
    "- for STS-B: [Pearson Correlation Coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) and [Spearman's_Rank_Correlation_Coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)\n",
    "- for WNLI: Accuracy\n",
    "\n",
    "so the metric object only computes the one(s) needed for your task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVx71GdAIrJH"
   },
   "source": [
    "Before we can feed those texts to our model, we need to preprocess them. This is done by a ðŸ¤— Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
    "\n",
    "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
    "\n",
    "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
    "- we download the vocabulary used when pretraining this specific checkpoint.\n",
    "\n",
    "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl6IidfdIrJK"
   },
   "source": [
    "We pass along `use_fast=True` to the call above to use one of the fast tokenizers (backed by Rust) from the ðŸ¤— Tokenizers library. Those fast tokenizers are available for almost all models, but if you got an error with the previous call, remove that argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rowT4iCLIrJK"
   },
   "source": [
    "You can directly call this tokenizer on one sentence or a pair of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "a5hBlsrHIrJL",
    "outputId": "acdaa98a-a8cd-4a20-89b8-cc26437bbe90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1010, 2023, 2028, 6251, 999, 102, 1998, 2023, 6251, 3632, 2007, 2009, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qo_0B1M2IrJM"
   },
   "source": [
    "Depending on the model you selected, you will see different keys in the dictionary returned by the cell above. They don't matter much for what we're doing here (just know they are required by the model we will instantiate later), you can learn more about them in [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested.\n",
    "\n",
    "To preprocess our dataset, we will thus need the names of the columns containing the sentence(s). The following dictionary keeps track of the correspondence task to column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "fyGdtK9oIrJM"
   },
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbqtC4MrIrJO"
   },
   "source": [
    "We can double check it does work on our current dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "19GG646uIrJO",
    "outputId": "0cb4a520-817e-4f92-8de8-bb45df367657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: hide new secretions from the parental units \n"
     ]
    }
   ],
   "source": [
    "sentence1_key, sentence2_key = task_to_keys[task]\n",
    "if sentence2_key is None:\n",
    "    print(f\"Sentence: {dataset['train'][0][sentence1_key]}\")\n",
    "else:\n",
    "    print(f\"Sentence 1: {dataset['train'][0][sentence1_key]}\")\n",
    "    print(f\"Sentence 2: {dataset['train'][0][sentence2_key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "We can then write the function that will preprocess our samples. We just feed them to the `tokenizer` with the three arguments.`truncation=True` will ensure that an input longer than maximum length will be truncated to the maximum length. `max_length=max_seq_length` sets the maximum length of a sequence.\n",
    "\n",
    "**Note: since we will use packing later, we don't want to perform any padding in the tokenizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vc0BSBLIIrJQ"
   },
   "outputs": [],
   "source": [
    "# no padding for packing\n",
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True, max_length=max_seq_length)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True, max_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lm8ozrJIrJR"
   },
   "source": [
    "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-b70jh26IrJS",
    "outputId": "acd3a42d-985b-44ee-9daa-af5d944ce1d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 5342, 2047, 3595, 8496, 2013, 1996, 18643, 3197, 102], [101, 3397, 2053, 15966, 1010, 2069, 4450, 2098, 18201, 2015, 102], [101, 2008, 7459, 2049, 3494, 1998, 10639, 2015, 2242, 2738, 3376, 2055, 2529, 3267, 102], [101, 3464, 12580, 8510, 2000, 3961, 1996, 2168, 2802, 102], [101, 2006, 1996, 5409, 7195, 1011, 1997, 1011, 1996, 1011, 11265, 17811, 18856, 17322, 2015, 1996, 16587, 2071, 2852, 24225, 2039, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(dataset['train'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "DDtsaJeVIrJT",
    "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandrec/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-82412aa6396b9933.arrow\n",
      "Loading cached processed dataset at /home/alexandrec/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d29a91fb10977f93.arrow\n",
      "Loading cached processed dataset at /home/alexandrec/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-af01fefce31fb4d7.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "67349"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "len(encoded_dataset['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voWiw8C7IrJV"
   },
   "source": [
    "Even better, the results are automatically cached by the ðŸ¤— Datasets library to avoid spending time on this step the next time you run your notebook. The ðŸ¤— Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). For instance, it will properly detect if you change the task in the first cell and rerun the notebook. ðŸ¤— Datasets warns you when it uses cached files, you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again.\n",
    "\n",
    "Note that we passed `batched=True` to encode the texts by batches together. This is to leverage the full benefit of the fast tokenizer we loaded earlier, which will use multi-threading to treat the texts in a batch concurrently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Packing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement packing, we need to pack our dataset first. Each new element will be a \"pack\" containing at most `max_seq_per_pack` sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_per_pack = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packing algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to pack efficiently, we will use an histogram-based algorithm (SPFHP) presented in the [blog post](https://www.graphcore.ai/posts/introducing-packed-bert-for-2x-faster-training-in-natural-language-processing) https://github.com/graphcore/tutorials/tree/master/blogs_code/packedBERT. First we need to generate the histogram of the sequences lengths in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_histogram(unpadded_input_ids, max_seq_len):\n",
    "    dataset_seq_lens:list = np.array([len(seq) for seq in unpadded_input_ids])\n",
    "    histogram = np.zeros(max_seq_len, dtype=np.int64)\n",
    "    seq_lens, counts = np.unique(dataset_seq_lens, return_counts=True)\n",
    "    histogram[seq_lens - 1] = counts\n",
    "\n",
    "    return histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "\n",
    "train_dataset = encoded_dataset['train']\n",
    "val_dataset = encoded_dataset[validation_key]\n",
    "\n",
    "train_hist = generate_histogram(train_dataset['input_ids'], max_seq_length )\n",
    "val_hist = generate_histogram(val_dataset['input_ids'], max_seq_length )\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_hist, bins = [k for k in range(0,max_seq_length,10)]) \n",
    "plt.title(\"sequences length histogram\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the `Shortest pack first histogram packing` algorithm to generate a packing strategy from the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from scipy import optimize, stats\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def add_pack(pack, count, tmp, final, limit, offset, max_sequence_length=512):\n",
    "    \"\"\"Filter out packs that reached maximum length or number of components.\"\"\"\n",
    "    if len(pack) == limit or offset == 0:\n",
    "        final[offset].append((count, pack))\n",
    "    else:\n",
    "        tmp[offset].append((count, pack))\n",
    "\n",
    "\n",
    "#^SPFHP - Shortest pack first histogram packing\n",
    "def SPFHP(histogram, max_sequence_length, max_sequences_per_pack):\n",
    "    \"\"\"Shortest-pack-first histogram-packing.\"\"\"\n",
    "    start = time.time()\n",
    "    reversed_histogram = np.flip(histogram)\n",
    "    # Initialize main strategy data dictionary.\n",
    "    # The key indicates how many tokens are left for full length.\n",
    "    # The value is a list of tuples, consisting of counts and respective packs.\n",
    "    # A pack is a (sorted) list of sequence length values that get concatenated.\n",
    "    tmp_strategies_per_length = defaultdict(list)\n",
    "    strategies_per_length = defaultdict(list)\n",
    "    # Index i indicates here, how much space is left, due to reversed histogram\n",
    "    for i in range(max_sequence_length):\n",
    "        n_sequences_to_bin = reversed_histogram[i]\n",
    "        length_to_bin = max_sequence_length - i\n",
    "        offset = i + 1  # largest possible offset\n",
    "        while n_sequences_to_bin > 0:\n",
    "            if (length_to_bin + offset) in tmp_strategies_per_length:\n",
    "                # extract shortest pack that will get modified\n",
    "                n_sequences_to_pack, pack = tmp_strategies_per_length[\n",
    "                    length_to_bin + offset].pop()\n",
    "                new_pack = pack + [length_to_bin]\n",
    "                count = min(n_sequences_to_pack, n_sequences_to_bin)\n",
    "                if n_sequences_to_pack > n_sequences_to_bin:\n",
    "                    # old pack gets reduced\n",
    "                    n_sequences_to_pack -= n_sequences_to_bin\n",
    "                    tmp_strategies_per_length[length_to_bin + offset].append(\n",
    "                        (n_sequences_to_pack, pack))\n",
    "                    n_sequences_to_bin = 0\n",
    "                else:\n",
    "                    n_sequences_to_bin -= n_sequences_to_pack\n",
    "                add_pack(new_pack, count,\n",
    "                         tmp_strategies_per_length, strategies_per_length,\n",
    "                         max_sequences_per_pack, offset)\n",
    "                # clean up to speed up main key search\n",
    "                if not tmp_strategies_per_length[length_to_bin + offset]:\n",
    "                    tmp_strategies_per_length.pop(length_to_bin + offset)\n",
    "            else:\n",
    "                offset -= 1\n",
    "            # Does not fit anywhere. Create new pack.\n",
    "            if offset < 0:\n",
    "                add_pack([length_to_bin], n_sequences_to_bin,\n",
    "                         tmp_strategies_per_length, strategies_per_length,\n",
    "                         max_sequences_per_pack, i)\n",
    "                n_sequences_to_bin = 0\n",
    "    # merge all strategies\n",
    "    for key in tmp_strategies_per_length:\n",
    "        strategies_per_length[key].extend(tmp_strategies_per_length[key])\n",
    "    # flatten strategies dictionary\n",
    "    strategy_set = []\n",
    "    strategy_repeat_count = []\n",
    "    for key in strategies_per_length:\n",
    "        for count, pack in strategies_per_length[key]:\n",
    "            pack.reverse()\n",
    "            strategy_set.append(pack)\n",
    "            strategy_repeat_count.append(count)\n",
    "\n",
    "    # Summarize efficiency of solution\n",
    "    duration = time.time() - start\n",
    "    sequence_lengths = np.arange(1, max_sequence_length + 1)\n",
    "    strategy_repeat_count = np.array(strategy_repeat_count)\n",
    "    n_strategies = len(strategy_set)\n",
    "    old_number_of_samples = histogram.sum()\n",
    "    new_number_of_samples = strategy_repeat_count.sum()\n",
    "    sequences = sum([count*len(pack) for count, pack in\n",
    "                     zip(strategy_repeat_count, strategy_set)])\n",
    "    total_tokens = max_sequence_length * new_number_of_samples\n",
    "    empty_tokens = sum([count*(max_sequence_length-sum(pack)) for count, pack\n",
    "                        in zip(strategy_repeat_count, strategy_set)])\n",
    "    efficiency = 100 - empty_tokens / total_tokens * 100\n",
    "    speedup_upper_bound = 1.0 / (1 - (histogram*(1 - sequence_lengths / max_sequence_length)).sum() / old_number_of_samples)\n",
    "    packing_factor = sequences/sum(strategy_repeat_count)\n",
    "    \n",
    "    print(f\"Packing efficiency (fraction of real tokens): {efficiency:3.4f}\\n\",\n",
    "          f\"Speed-up theoretical limit: {speedup_upper_bound:3.4f}\\n\",\n",
    "          f\"Achieved speed-up over un-packed dataset: {old_number_of_samples/new_number_of_samples:3.5f}\\n\",\n",
    "          f\"Runtime: Packed {old_number_of_samples} sequences in {duration:3.3f} seconds\\n\",\n",
    "          f\"Average packing factor: {packing_factor}\")\n",
    "    \n",
    "\n",
    "    return strategy_set, np.array(strategy_repeat_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`strategy_set` is a list of lists containing the sequences lenghts we can pack together.\n",
    "\n",
    "`strategy_repeat_count` gives the corresponding number of time we can create each pack of `strategy_set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packing efficiency (fraction of real tokens): 26.8058\n",
      " Speed-up theoretical limit: 19.2203\n",
      " Achieved speed-up over un-packed dataset: 5.15216\n",
      " Runtime: Packed 67349 sequences in 0.002 seconds\n",
      " Average packing factor: 5.152157282741738\n",
      "Packing efficiency (fraction of real tokens): 56.7648\n",
      " Speed-up theoretical limit: 10.1733\n",
      " Achieved speed-up over un-packed dataset: 5.77483\n",
      " Runtime: Packed 872 sequences in 0.002 seconds\n",
      " Average packing factor: 5.774834437086093\n"
     ]
    }
   ],
   "source": [
    "train_strategy = SPFHP(train_hist, max_seq_length, max_seq_per_pack)\n",
    "val_strategy = SPFHP(val_hist, max_seq_length, max_seq_per_pack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create the actual packed dataset object. \n",
    "We pick the sequences and pack them based on their length and following the strategy we just generated. Once they are packed, we also need to pad the sequences to the `max_seq_lentgh` to maintain a constant input size.\n",
    "\n",
    "Notes:\n",
    "- A specific `attention_mask` is generated: It contains a unique index for each sequence of the pack and `0` for the remaining padding tokens.\n",
    "    - Example of 3 sequences: `attention_mask = [1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,0,...,0,1,2,3]`\n",
    "\n",
    "\n",
    "- The [CLS] tokens of each sequence are moved at the end of the pack.\n",
    "    - For instance: `[CLS,a,b,c] + [CLS, d,e,f] + [CLS, g,h,i] -> [a,b,c,d,e,f,g,h,i,...,CLS,CLS,CLS]`\n",
    "    \n",
    "\n",
    "- The `position_ids` of a pack contains the concatenated `position_ids` of each sequences \n",
    "    - For instance given 3 sequences: `[0,1,2,3,4] + [0,1,2,3] + [0,1,2] -> [1,2,3,4,1,2,3,1,2,...,0,0,0]` (note: the CLS tokens position id '0' are also moved the end of the pack)\n",
    "    \n",
    "- `labels` and `token_type_ids` are also packed to correspond the `input_ids` pack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def create_dataset_from_strategy(data, strategy_set, strategy_repeat_count, max_seq_len, max_seq_per_pack):\n",
    "    total_num_packs:int = np.sum(strategy_repeat_count)\n",
    "        \n",
    "\n",
    "    # Sort the sequences by length\n",
    "    dataset_seq_lens = np.array([len(seq) for seq in data['input_ids']])\n",
    "    len_sorted_seq_idxs = np.argsort(dataset_seq_lens)\n",
    "    len_sorted_seq_lens = dataset_seq_lens[len_sorted_seq_idxs]\n",
    "    sorted_seqs = np.stack((len_sorted_seq_lens, len_sorted_seq_idxs))\n",
    "\n",
    "\n",
    "    # Get the data from the tokenised dataset\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    labels = data['label']\n",
    "    \n",
    "    # Prepare the manually padded constant sized data\n",
    "    packed_input_ids = np.zeros((total_num_packs, max_seq_len), dtype=int)\n",
    "    packed_attention_mask = np.zeros((total_num_packs, max_seq_len), dtype=int)\n",
    "    packed_token_type_ids = np.zeros((total_num_packs, max_seq_len), dtype=int)\n",
    "    packed_position_ids = np.zeros((total_num_packs, max_seq_len), dtype=int)\n",
    "    packed_labels = -100 * np.ones((total_num_packs, max_seq_per_pack), dtype=int)\n",
    "    \n",
    "    # Pack the data using the developed strategies\n",
    "    pack_index = 0\n",
    "    for i in range(len(strategy_repeat_count)):\n",
    "        strategy = strategy_set[i]\n",
    "        # This is the offset we apply to the start positions to account for the positional change of the logits when unmasking the pack to extract a set of logits for each sequence in the pack\n",
    "        for _ in range(strategy_repeat_count[i]):\n",
    "\n",
    "            '''Key terms in loop:\n",
    "\n",
    "            * sorted_seqs: (shape [2, dataset])\n",
    "                - index 0: sorted lengths of each sequence in dataset\n",
    "                    -- e.g. sorted_seqs[0,12] gives the length of the sequence at dataset position at index: sorted_seqs[1,12]\n",
    "                - index 1: index of corresponding lengths in the dataset\n",
    "                    -- e.g. dataset[sorted_seqs[1,12]] returns dataset sequence at index: sorted_seqs[1,12]\n",
    "\n",
    "            * ref_inds: (shape [strategy_set])\n",
    "                - the indices of the [length, dataset index] pair in sorted_seqs (this is used to remove/clear sorted_seqs as data is packed).\n",
    "                    -- e.g sorted_seqs[0, ref_inds] = -1 will nullify the sequence length at positions in [array] ref_inds such that they cannot be called to pull data from those indices again.\n",
    "\n",
    "            * inds: (shape [strategy_set])\n",
    "                - the indices in the actual dataset, called using the indices of sorted_seqs retrieved from ref_inds.\n",
    "                    --e.g. > inds = sorted_seqs[1, ref_inds]\n",
    "                           > packed data = concatenate(dataset[inds])\n",
    "            '''\n",
    "\n",
    "            ref_inds = []\n",
    "            for x in strategy:\n",
    "                ref_ind = np.argwhere(sorted_seqs[0] == x)[-1]\n",
    "                sorted_seqs[0, ref_ind] = -1\n",
    "                ref_inds.append(ref_ind)\n",
    "\n",
    "            inds = sorted_seqs[1, ref_inds].ravel()\n",
    "\n",
    "            # Exclude the CLS tokens to put them at the end later\n",
    "            input_id_pack = list(itertools.chain(*[input_ids[x][1:] for x in inds]))\n",
    "            attention_mask_pack = list(itertools.chain(*[itertools.repeat(n+1, len(attention_mask[v])-1) for n,v in enumerate(inds)]))\n",
    "            token_type_ids_pack = list(itertools.chain(*[token_type_ids[x][1:] for x in inds]))\n",
    "            position_ids_pack = list(itertools.chain(*[range(1, len(attention_mask[v])) for n,v in enumerate(inds)]))\n",
    "\n",
    "            # Create the equivalent tokenised packed dataset\n",
    "            packed_input_ids[pack_index, :len(input_id_pack)] = input_id_pack\n",
    "            packed_attention_mask[pack_index, :len(attention_mask_pack)] = attention_mask_pack\n",
    "            packed_token_type_ids[pack_index, :len(token_type_ids_pack)] = token_type_ids_pack\n",
    "            packed_position_ids[pack_index, :len(position_ids_pack)] = position_ids_pack\n",
    "            labels_pack = [labels[x] for x in inds]\n",
    "            packed_labels[pack_index, :len(labels_pack)] = labels_pack\n",
    "\n",
    "            # Now add the CLS tokens and their masks at the end of the pack\n",
    "            packed_input_ids[pack_index, -max_seq_per_pack:] = [input_ids[0][0] for _ in range(max_seq_per_pack)]\n",
    "            packed_attention_mask[pack_index, -max_seq_per_pack:] = list(range(1, max_seq_per_pack+1))\n",
    "\n",
    "            pack_index += 1\n",
    "            \n",
    "    new_dataset = Dataset.from_dict({ \"input_ids\": packed_input_ids,\n",
    "                                      \"attention_mask\": packed_attention_mask,\n",
    "                                      \"token_type_ids\": packed_token_type_ids,\n",
    "                                      \"position_ids\": packed_position_ids,\n",
    "                                      \"labels\": packed_labels\n",
    "                                })\n",
    "    new_dataset.set_format(type='torch', columns=new_dataset.features)\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'token_type_ids', 'position_ids', 'labels'],\n",
      "    num_rows: 13072\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "packed_train_dataset = create_dataset_from_strategy(train_dataset, train_strategy[0], train_strategy[1], max_seq_length, max_seq_per_pack)\n",
    "packed_val_dataset = create_dataset_from_strategy(val_dataset, val_strategy[0], val_strategy[1], max_seq_length, max_seq_per_pack)\n",
    "\n",
    "print(packed_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize one sample of the new `packed_train_dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([13401,  1005,  1055, 22391,  2104, 10841, 14343,  3372,  1997, 18439,\n",
       "          1998,  5988, 16778,  2278, 22012,   102,  1996,  3494,  1010,  2040,\n",
       "          2024,  2061, 19337,  2666, 12423,  2008,  2017,  2514,  2054,  2027,\n",
       "          2514,   102,  1996,  3609,  3168,  1997,  6990,  2210,  1016,  2003,\n",
       "          2049,  2087,  6234,  1998,  2087,  5793,  5165,   102,  3310,  2013,\n",
       "          1996,  9191,  1010,  4895,  2378,  4048, 16313,  2098,  4616,  2011,\n",
       "          2049,  2599,  5889,   102,  1005,  2310,  2464,  1999,  1037,  2096,\n",
       "          1010,  1037,  2812,  4063,  2083,  6247,  1011,  2041,  3430,   102,\n",
       "         11896,  2000,  3073,  2172,  2062, 12369,  2084,  1996,  2503,  5930,\n",
       "          1997,  1037,  7950,  2338,  6598,  1012,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           101,   101,   101,   101,   101,   101]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "         4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "         5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "         6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'position_ids': tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  1,  2,\n",
       "          3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  1,  2,  3,  4,\n",
       "          5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  1,  2,  3,  4,  5,  6,\n",
       "          7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  1,  2,  3,  4,  5,  6,  7,  8,\n",
       "          9, 10, 11, 12, 13, 14, 15, 16,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10,\n",
       "         11, 12, 13, 14, 15, 16, 17,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0]),\n",
       " 'labels': tensor([1, 1, 1, 1, 0, 0])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_train_dataset[3020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "Now that our data is ready, we can download the pretrained model and fine-tune it. The number of labels will be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, default_data_collator\n",
    "from optimum.graphcore import IPUConfig, IPUTrainer, IPUTrainingArguments\n",
    "\n",
    "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Packed BERT\n",
    "\n",
    "A few model modifications are required to make packing work with BERT.\n",
    "We will extend the existing class `BertForSequenceClassification`.\n",
    "\n",
    "First let's load a default BERT configuration using `AutoConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 256,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "config.max_position_embeddings = max_seq_length\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packing sequences increases the number of elements per batch.\n",
    "In order to reuse the classifications heads from `transformers` library, we need a special pooler. Instead of pooling the hidden states of a single sequence, it's pooling multiple ones (given the maxium number of sequences in the pack) and ordering them along the batch dimension. So the output size of the pooler is: `[batch-size x max_sequences_per_pack, hidden_size]`\n",
    "\n",
    "From the Loss point-of-view , everything will appear as if the batch-size was larger (`batch-size x max_sequences_per_pack`).\n",
    "When the number of sequences in the pack is lower than `max_sequences_per_pack`, padding is ignored by using the default `ignore_index` (-100) of the loss as a special labels (this was already done in the dataset preprocessing, cf: *Packing the dataset*).\n",
    "\n",
    "![pooling](images/pooling.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PackedBertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.max_sequences_per_pack = config.max_sequences_per_pack\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden states corresponding\n",
    "        # to the last max_sequences_per_pack tokens. Note that the [CLS] tokens\n",
    "        # are always located at the end of the pack. When the actual number of\n",
    "        # sequences is lower than max_sequences_per_pack, we still slice out\n",
    "        # the last max_sequences_per_pack tokens, but we will not use all of\n",
    "        # them during loss calculation.\n",
    "        sh = hidden_states.shape\n",
    "        last_tokens_tensors = hidden_states[:, -self.max_sequences_per_pack:]\n",
    "        last_reshape = last_tokens_tensors.reshape(sh[0]*self.max_sequences_per_pack, sh[2])\n",
    "        # output size: [bs x max_sequences_per_pack, hidden_size]\n",
    "        output = self.dense(last_reshape)\n",
    "        output = self.activation(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attention mask\n",
    "The attention mask should be used in a specific way in packed-BERT.\n",
    "We will create a 2D attention mask like in the following example.\n",
    "By doing so, the cross-attention will treat separately each sequence of the pack (and it will also ignore the padding).\n",
    "![attn-mask](images/attention-mask.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better intuition here is an example showing how to transform the 1D attention mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "         [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "         [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "         [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "         [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "         [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "         [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "         [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "         [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1]]])\n"
     ]
    }
   ],
   "source": [
    "# 1 : Flat attention mask genreated by the dataset. Each sequence has a different index. 0 is padding.\n",
    "attention_mask = torch.tensor([[1,1,2,2,3,3,3,4,4,4,4,0,0,0,0,1,2,3,4]])\n",
    "# 2: Generate the boolean 2D attention mask\n",
    "attention_mask = attention_mask[:, None, :].repeat(1, attention_mask.shape[1], 1)\n",
    "attention_mask = (attention_mask == attention_mask.transpose(1, 2)) * (attention_mask != 0)\n",
    "# Notice that the mask is always False for the padding tokens.\n",
    "print(attention_mask.to(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's integrate this idea to the input of packed BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inheriting from `BertPipelineMixin` , the `paralellize()` method is already implemented for the BERT body. We overloaded it to also place the classifier on the last IPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import poptorch\n",
    "from optimum.graphcore.models.bert.modeling_bert import BertPipelineMixin\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "\n",
    "class PackedBertForSequenceClassification(BertForSequenceClassification, BertPipelineMixin):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config.max_sequences_per_pack = max_seq_per_pack\n",
    "        self.bert.pooler = PackedBertPooler(config)\n",
    "        \n",
    "    def parallelize(self):\n",
    "            super().parallelize()\n",
    "            last_ipu = self.ipu_config.ipus_per_replica - 1\n",
    "            self.classifier = poptorch.BeginBlock(self.classifier, \"Classifier Output\", ipu_id=last_ipu)\n",
    "            return self\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, labels=None):\n",
    "        \n",
    "        seq_len = input_ids.shape[1]\n",
    "        attention_mask = attention_mask[:, None, :].repeat(1, seq_len, 1)\n",
    "        attention_mask = (attention_mask == attention_mask.transpose(1, 2)) * (attention_mask != 0)\n",
    "        \n",
    "        output = super().forward(input_ids = input_ids,\n",
    "                                 attention_mask=attention_mask,\n",
    "                                 token_type_ids=token_type_ids,\n",
    "                                 position_ids=position_ids,\n",
    "                                 labels=labels)\n",
    "\n",
    "        # For validation: output should keep the same batch dimension as the original input\n",
    "        if not self.training:\n",
    "            output.logits = output.logits.reshape([-1,max_seq_per_pack, num_labels])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing PackedBertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing PackedBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing PackedBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of PackedBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = PackedBertForSequenceClassification(config).from_pretrained(\"bert-base-uncased\", num_labels=num_labels).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CczA5lJlIrJX"
   },
   "source": [
    "The warning is telling us we are throwing away some weights and randomly initializing some other. This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first test the model on CPU and observe that the output logits have now the size [batch_size x max_seq_per_pack, 2] = [12, 2] with this notebook default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:  SequenceClassifierOutput(loss=tensor(0.7834, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1241, -0.5591],\n",
      "        [ 0.0029, -0.5513],\n",
      "        [ 0.0344, -0.4092],\n",
      "        [-0.2084, -0.8723],\n",
      "        [-0.1712, -0.3608],\n",
      "        [ 0.0211, -0.5014],\n",
      "        [-0.1167, -0.7668],\n",
      "        [-0.1232, -0.6668],\n",
      "        [-0.1602, -0.9111],\n",
      "        [-0.0286, -0.5740],\n",
      "        [ 0.1363, -0.1791],\n",
      "        [-0.2363, -0.8429]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers.data.data_collator import default_data_collator\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(packed_train_dataset,\n",
    "                             batch_size=micro_batch_size,\n",
    "                             shuffle=True,\n",
    "                             drop_last=True,\n",
    "                             collate_fn=default_data_collator)\n",
    "data = iter(loader).next()\n",
    "outputs = model(**data)\n",
    "print(\"logits: \", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's prepare the model for IPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set the model in half precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedBertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): PackedBertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.half()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "We need to define the `IPUConfig`, which is a class that specifies attributes and configuration parameters to compile and put the model on the device. We initialize it with one config name or path, which we set earlier. Then we use it to set the mode attribute `model.ipu_config` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localdata/alexandrec/SDK/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages/optimum/graphcore/ipu_configuration.py:148: UserWarning: The \"enable_half_first_order_momentum\" parameter is deprecated\n",
      "  warnings.warn('The \"enable_half_first_order_momentum\" parameter is deprecated')\n"
     ]
    }
   ],
   "source": [
    "ipu_config = IPUConfig.from_pretrained(\n",
    "    ipu_config_name,\n",
    "    executable_cache_dir = \"/tmp/exe_cache/\",\n",
    "    replication_factor=1,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    device_iterations = 32,\n",
    "    inference_replication_factor=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For validation, we need to define a function to compute the metrics from the predictions, which will just use the `metric` we loaded earlier, the only preprocessing we have to do is to take the argmax of our predicted logits (our just squeeze the last axis in the case of STS-B). To ignore the `-100` labels from uncomplete packs, we use a boolean mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "#     Remove the padding labels\n",
    "    mask = (labels != -100)\n",
    "    labels = labels[mask]\n",
    "    predictions = predictions[mask]\n",
    "    if task != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Setting replicated_tensor_sharding to False when replication_factor=1\n",
      "Overriding IPU config: gradient_accumulation_steps=32\n",
      "-------------------- Device Allocation --------------------\n",
      "Embedding --> IPU 0\n",
      "Encoder 0  --> IPU 1\n",
      "Encoder 1  --> IPU 1\n",
      "Encoder 2  --> IPU 1\n",
      "Encoder 3  --> IPU 1\n",
      "Encoder 4  --> IPU 2\n",
      "Encoder 5  --> IPU 2\n",
      "Encoder 6  --> IPU 2\n",
      "Encoder 7  --> IPU 2\n",
      "Encoder 8  --> IPU 3\n",
      "Encoder 9  --> IPU 3\n",
      "Encoder 10 --> IPU 3\n",
      "Encoder 11 --> IPU 3\n"
     ]
    }
   ],
   "source": [
    "args = IPUTrainingArguments(\n",
    "    \"/tmp/\"+f\"{model_name}-finetuned-{task}\",\n",
    "    learning_rate=0.00009,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=micro_batch_size,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0,\n",
    "    metric_for_best_model=metric_name,\n",
    "    dataloader_drop_last=True,\n",
    "    dataloader_mode=\"async_rebatched\",\n",
    "    logging_steps=1,\n",
    "    pod_type=pod_type,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    push_to_hub=False,\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "trainer = IPUTrainer(\n",
    "    model,\n",
    "    ipu_config,\n",
    "    args,\n",
    "    train_dataset=packed_train_dataset,\n",
    "    eval_dataset=packed_val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "[15:03:53.208] [poptorch:cpp] [warning] [TRACING-2] Allocated tensor: 13 sizes=[1, 512], type=Int (type coerced from Long to Int)\n",
      "[15:03:53.208] [poptorch:cpp] [warning] [TRACING-2] Allocated tensor: 15 sizes=[1, 512], type=Int (type coerced from Long to Int)\n",
      "[15:03:53.328] [poptorch:cpp] [warning] Parameter bert.embeddings.position_ids: impl_ 0x88b36b0 type xla ID 14 sizes [1, 512] dtype int was downgraded to constant because PopART doesn't support non floating point parameters\n",
      "[15:03:53.328] [poptorch:cpp] [warning] Parameter bert.embeddings.token_type_ids: impl_ 0x8862cb0 type xla ID 16 sizes [1, 512] dtype int was downgraded to constant because PopART doesn't support non floating point parameters\n",
      "[15:03:53.334] [poptorch:cpp] [warning] [TRACING-2] Allocated tensor: 409 sizes=[2, 256], type=Int (type coerced from Long to Int)\n",
      "[15:03:53.334] [poptorch:cpp] [warning] [TRACING-2] Allocated tensor: 411 sizes=[2, 256], type=Int (type coerced from Long to Int)\n",
      "[15:03:53.334] [poptorch:cpp] [warning] [TRACING-2] Allocated tensor: 413 sizes=[2, 256], type=Int (type coerced from Long to Int)\n",
      "[15:03:53.334] [poptorch:cpp] [warning] [TRACING-2] Allocated tensor: 415 sizes=[2, 256], type=Int (type coerced from Long to Int)\n",
      "[15:03:53.335] [poptorch:cpp] [warning] [TRACING-2] Allocated tensor: 417 sizes=[2, 6], type=Int (type coerced from Long to Int)\n",
      "[15:03:53.338] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x8a6b640) type coerced from Double to Float\n",
      "[15:03:53.338] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x8a6b640) type coerced from Double to Float\n",
      "[15:03:53.339] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x37c8e9d0) type coerced from Long to Int\n",
      "[15:03:53.342] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x37c9a8d0) type coerced from Long to Int\n",
      "[15:03:53.347] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x3ff00b00) type coerced from Double to Float\n",
      "[15:03:53.354] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x3d1e4810) type coerced from Double to Float\n",
      "[15:03:53.361] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x3e24cb60) type coerced from Double to Float\n",
      "[15:03:53.368] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x3e292a30) type coerced from Double to Float\n",
      "[15:03:53.376] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x3cbf21c0) type coerced from Double to Float\n",
      "[15:03:53.383] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x3e8586c0) type coerced from Double to Float\n",
      "[15:03:53.390] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x3e899e10) type coerced from Double to Float\n",
      "[15:03:53.398] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x3bb7ac60) type coerced from Double to Float\n",
      "[15:03:53.405] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x3885c6e0) type coerced from Double to Float\n",
      "[15:03:53.412] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x388a5d90) type coerced from Double to Float\n",
      "[15:03:53.419] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x3b583740) type coerced from Double to Float\n",
      "[15:03:53.426] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x3ee65100) type coerced from Double to Float\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:14<00:00]\n",
      "Compiled/Loaded model in 28.963948167976923 secs\n",
      "***** Running training *****\n",
      "  Num examples = 13072\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2048\n",
      "  Gradient Accumulation steps = 32\n",
      "  Total optimization steps = 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0cb254a9de5441f8efd702f491bae49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7256, 'learning_rate': 4.5e-05, 'epoch': 0.17}\n",
      "{'loss': 0.5747, 'learning_rate': 9e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0968, 'learning_rate': 8.779754323328193e-05, 'epoch': 0.5}\n",
      "{'loss': 0.1924, 'learning_rate': 8.140576474687264e-05, 'epoch': 0.67}\n",
      "{'loss': 0.2295, 'learning_rate': 7.14503363531613e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0856, 'learning_rate': 5.890576474687264e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0141, 'learning_rate': 4.5e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0443, 'learning_rate': 3.1094235253127374e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0082, 'learning_rate': 1.8549663646838714e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0213, 'learning_rate': 8.59423525312737e-06, 'epoch': 1.67}\n",
      "{'loss': 0.4536, 'learning_rate': 2.2024567667180914e-06, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0236, 'learning_rate': 0.0, 'epoch': 2.0}\n",
      "{'train_runtime': 36.3568, 'train_samples_per_second': 675.967, 'train_steps_per_second': 0.33, 'train_loss': 0.20580355326334634, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12, training_loss=0.20580355326334634, metrics={'train_runtime': 36.3568, 'train_samples_per_second': 675.967, 'train_steps_per_second': 0.33, 'train_loss': 0.20580355326334634, 'epoch': 2.0})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***About the performances:*** `IPUTrainer` doesn't take into account that we have packed data samples when computing the speed metrics. So the actual throughput estimation can be obtained by multiplying the `samples_per_second` by the average packing factor of the dataset. (These were obtained in the `packing_algorithm` section: `5.15` for `sst2` training set and `5.77` for validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "[15:05:02.621] [poptorch:cpp] [warning] [TRACING-2] Allocated tensor: 13 sizes=[1, 512], type=Int (type coerced from Long to Int)\n",
      "[15:05:02.621] [poptorch:cpp] [warning] [TRACING-2] Allocated tensor: 15 sizes=[1, 512], type=Int (type coerced from Long to Int)\n",
      "[15:05:02.716] [poptorch:cpp] [warning] Parameter bert.embeddings.position_ids: impl_ 0xc800c50 type xla ID 14 sizes [1, 512] dtype int was downgraded to constant because PopART doesn't support non floating point parameters\n",
      "[15:05:02.716] [poptorch:cpp] [warning] Parameter bert.embeddings.token_type_ids: impl_ 0x3e8897c0 type xla ID 16 sizes [1, 512] dtype int was downgraded to constant because PopART doesn't support non floating point parameters\n",
      "[15:05:02.718] [poptorch:cpp] [warning] [TRACING-2] Allocated tensor: 409 sizes=[4, 256], type=Int (type coerced from Long to Int)\n",
      "[15:05:02.718] [poptorch:cpp] [warning] [TRACING-2] Allocated tensor: 411 sizes=[4, 256], type=Int (type coerced from Long to Int)\n",
      "[15:05:02.719] [poptorch:cpp] [warning] [TRACING-2] Allocated tensor: 413 sizes=[4, 256], type=Int (type coerced from Long to Int)\n",
      "[15:05:02.719] [poptorch:cpp] [warning] [TRACING-2] Allocated tensor: 415 sizes=[4, 256], type=Int (type coerced from Long to Int)\n",
      "[15:05:02.719] [poptorch:cpp] [warning] [TRACING-2] Allocated tensor: 417 sizes=[4, 6], type=Int (type coerced from Long to Int)\n",
      "[15:05:02.721] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x3cbedf20) type coerced from Double to Float\n",
      "[15:05:02.721] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x3cbedf20) type coerced from Double to Float\n",
      "[15:05:02.722] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x46ad9880) type coerced from Long to Int\n",
      "[15:05:02.723] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x46adc420) type coerced from Long to Int\n",
      "[15:05:02.728] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x4a3f34c0) type coerced from Double to Float\n",
      "[15:05:02.736] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x73639790) type coerced from Double to Float\n",
      "[15:05:02.743] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x53b109a0) type coerced from Double to Float\n",
      "[15:05:02.750] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x59cf8360) type coerced from Double to Float\n",
      "[15:05:02.757] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x1ca9aa0c0) type coerced from Double to Float\n",
      "[15:05:02.763] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x50570840) type coerced from Double to Float\n",
      "[15:05:02.770] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x4d2171b0) type coerced from Double to Float\n",
      "[15:05:02.777] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x58c6a430) type coerced from Double to Float\n",
      "[15:05:02.785] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x4c056050) type coerced from Double to Float\n",
      "[15:05:02.792] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x4c09ad60) type coerced from Double to Float\n",
      "[15:05:02.799] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x498070a0) type coerced from Double to Float\n",
      "[15:05:02.806] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x45471b60) type coerced from Double to Float\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:04<00:00]\n",
      "Compiled/Loaded model in 16.023812644998543 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 151\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288bd047a841409885c800265bba4aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.220947265625,\n",
       " 'eval_accuracy': 0.9199522102747909,\n",
       " 'eval_runtime': 0.7446,\n",
       " 'eval_samples_per_second': 193.403,\n",
       " 'eval_steps_per_second': 12.088,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how your model fared you can compare it to the [GLUE Benchmark leaderboard](https://gluebenchmark.com/leaderboard).\n",
    "\n",
    "You can now upload the result of the training to the Hub, just execute this instruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now share this model with all your friends, family, favorite pets: they can all load it with the identifier `\"your-username/the-name-you-picked\"` so for instance:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"sgugger/my-awesome-model\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster inference:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training, the packing factor does affect the convergence the same way as bigger batch size would do. However, for inference, we are free to use a bigger packing factor to speed it up.\n",
    "Let's try it on `sst2` with `max_seq_per_pack = 12`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_per_pack = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have enough examples, we will reuse the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/alexandrec/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec2b0740fbd41f88a13ed6cca7b40ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandrec/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-82412aa6396b9933.arrow\n",
      "Loading cached processed dataset at /home/alexandrec/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d29a91fb10977f93.arrow\n",
      "Loading cached processed dataset at /home/alexandrec/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-af01fefce31fb4d7.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packing efficiency (fraction of real tokens): 34.9567\n",
      " Speed-up theoretical limit: 19.2203\n",
      " Achieved speed-up over un-packed dataset: 6.71877\n",
      " Runtime: Packed 67349 sequences in 0.002 seconds\n",
      " Average packing factor: 6.718774940143655\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "inference_dataset = encoded_dataset['train'] # train set again, to have enough examples\n",
    "infer_strategy = SPFHP(train_hist, max_seq_length, max_seq_per_pack)\n",
    "packed_dataset = create_dataset_from_strategy(train_dataset, infer_strategy[0], infer_strategy[1], max_seq_length, max_seq_per_pack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the average packing factor `6.7` is not close to the maximum now (12), this is still an imporvement compared to the previous `5.7`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also modify the configuration of the model for inference. For speed up, we can us a single IPU and 4 replicas by changing `layers_per_ipu` , `inference_replication_factor` and `ipus_per_replica` and also use a larger `batch-size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipu_config.layers_per_ipu = [12]\n",
    "ipu_config.inference_device_iterations = 32\n",
    "ipu_config.inference_replication_factor = 4\n",
    "ipu_config.ipus_per_replica = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing PackedBertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing PackedBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing PackedBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of PackedBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = PackedBertForSequenceClassification(config).from_pretrained(\"bert-base-uncased\", num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Setting replicated_tensor_sharding to False when replication_factor=1\n",
      "-------------------- Device Allocation --------------------\n",
      "Embedding --> IPU 0\n",
      "Encoder 0  --> IPU 0\n",
      "Encoder 1  --> IPU 0\n",
      "Encoder 2  --> IPU 0\n",
      "Encoder 3  --> IPU 0\n",
      "Encoder 4  --> IPU 0\n",
      "Encoder 5  --> IPU 0\n",
      "Encoder 6  --> IPU 0\n",
      "Encoder 7  --> IPU 0\n",
      "Encoder 8  --> IPU 0\n",
      "Encoder 9  --> IPU 0\n",
      "Encoder 10 --> IPU 0\n",
      "Encoder 11 --> IPU 0\n"
     ]
    }
   ],
   "source": [
    "args = IPUTrainingArguments(\n",
    "    \"/tmp/\"+f\"{model_name}-finetuned-{task}-fast-inference\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    dataloader_mode=\"async_rebatched\",\n",
    "    dataloader_drop_last=True,\n",
    "    logging_steps=10,\n",
    "    pod_type=pod_type\n",
    ")\n",
    "\n",
    "trainer = IPUTrainer(\n",
    "    model,\n",
    "    ipu_config,\n",
    "    args,\n",
    "    eval_dataset=packed_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling Model...\n",
      "[15:08:30.589] [poptorch:cpp] [warning] [TRACING-2] Allocated tensor: 13 sizes=[1, 512], type=Int (type coerced from Long to Int)\n",
      "[15:08:30.590] [poptorch:cpp] [warning] [TRACING-2] Allocated tensor: 15 sizes=[1, 512], type=Int (type coerced from Long to Int)\n",
      "[15:08:30.754] [poptorch:cpp] [warning] Parameter bert.embeddings.position_ids: impl_ 0x1f4320f30 type xla ID 14 sizes [1, 512] dtype int was downgraded to constant because PopART doesn't support non floating point parameters\n",
      "[15:08:30.754] [poptorch:cpp] [warning] Parameter bert.embeddings.token_type_ids: impl_ 0x1ca9c1640 type xla ID 16 sizes [1, 512] dtype int was downgraded to constant because PopART doesn't support non floating point parameters\n",
      "[15:08:30.755] [poptorch:cpp] [warning] [TRACING-2] Allocated tensor: 409 sizes=[8, 256], type=Int (type coerced from Long to Int)\n",
      "[15:08:30.756] [poptorch:cpp] [warning] [TRACING-2] Allocated tensor: 411 sizes=[8, 256], type=Int (type coerced from Long to Int)\n",
      "[15:08:30.756] [poptorch:cpp] [warning] [TRACING-2] Allocated tensor: 413 sizes=[8, 256], type=Int (type coerced from Long to Int)\n",
      "[15:08:30.756] [poptorch:cpp] [warning] [TRACING-2] Allocated tensor: 415 sizes=[8, 256], type=Int (type coerced from Long to Int)\n",
      "[15:08:30.756] [poptorch:cpp] [warning] [TRACING-2] Allocated tensor: 417 sizes=[8, 12], type=Int (type coerced from Long to Int)\n",
      "[15:08:30.758] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x16ad6f8e0) type coerced from Double to Float\n",
      "[15:08:30.759] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x16ad6f8e0) type coerced from Double to Float\n",
      "[15:08:30.759] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x864e230) type coerced from Long to Int\n",
      "[15:08:30.760] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x3ff0d510) type coerced from Long to Int\n",
      "[15:08:30.766] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x3e8555c0) type coerced from Double to Float\n",
      "[15:08:30.773] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x50551180) type coerced from Double to Float\n",
      "[15:08:30.781] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x59cd8520) type coerced from Double to Float\n",
      "[15:08:30.789] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x8899dc0) type coerced from Double to Float\n",
      "[15:08:30.797] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x53aed760) type coerced from Double to Float\n",
      "[15:08:30.805] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x5864c770) type coerced from Double to Float\n",
      "[15:08:30.813] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x38873230) type coerced from Double to Float\n",
      "[15:08:30.821] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0xf778c30) type coerced from Double to Float\n",
      "[15:08:30.829] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x4980cab0) type coerced from Double to Float\n",
      "[15:08:30.837] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x4d234780) type coerced from Double to Float\n",
      "[15:08:30.844] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x46b003a0) type coerced from Double to Float\n",
      "[15:08:30.852] [poptorch:cpp] [warning] [TRACING-2] Tensor (ptr 0x3e85fba0) type coerced from Double to Float\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00]\n",
      "WARNING: The compile time engine option debug.branchRecordTile is set to \"5887\" when creating the Engine. (At compile-tile it was set to 1471)\n",
      "Compiled/Loaded model in 14.623454237007536 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10024\n",
      "  Batch size = 1024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383b2aac21ac4739a9384852f37fa4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.828125,\n",
       " 'eval_accuracy': 0.4437953539318151,\n",
       " 'eval_runtime': 5.0264,\n",
       " 'eval_samples_per_second': 1833.521,\n",
       " 'eval_steps_per_second': 1.791}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, to get a correct throughput estimation we need to multiply `eval_samples_per_second` by the average packing factor (6.72)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Text Classification on GLUE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
